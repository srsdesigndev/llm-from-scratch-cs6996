# LLM From Scratch

Building a Large Language Model from scratch following Sebastian Raschka's book "Build a Large Language Model (From Scratch)".

**Student:** Sundar Raj Sharma  
**Advisor:** Dr. Feng (George) Yu  
**Course:** CS-6996 Independent Study  
**Institution:** Youngstown State University

---

## ðŸ““ Notebook

**[llm-scratch-is-6996.ipynb](llm-scratch-is-6996.ipynb)** - Complete implementation  
ðŸ”— [Open in Colab](https://colab.research.google.com/github/YOUR-USERNAME/llm-from-scratch-is-6996/blob/main/llm-scratch-is-6996.ipynb)

---

## ðŸ“š Book

**Build a Large Language Model (From Scratch)**  
Author: Sebastian Raschka  
Publisher: Manning Publications  
[Read on O'Reilly](https://learning.oreilly.com/library/view/build-a-large/9781633437166/)

---

## ðŸŽ¯ What I'm Building

Implementing all core LLM components from scratch:
- Tokenization (BPE)
- Embeddings & positional encoding
- Self-attention mechanisms
- Multi-head attention
- Transformer blocks
- GPT architecture
- Training & fine-tuning

---

## ðŸ’» Tech Stack

- Python 3.x
- PyTorch
- Google Colab
- NumPy, Matplotlib

---

## ðŸ”— Resources

- [Author's GitHub](https://github.com/rasbt/LLMs-from-scratch)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [PyTorch Docs](https://pytorch.org/docs/stable/index.html)

---

## ðŸ“§ Contact

**Student:** ssharma33@student.ysu.edu  
**Advisor:** fyu@ysu.edu

---

**Last Updated:** February 12, 2025