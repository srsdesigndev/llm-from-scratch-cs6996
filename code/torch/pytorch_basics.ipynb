{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [

  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# PyTorch — Complete Foundations\n",
    "\n",
    "PyTorch is the dominant framework for deep learning research and production. Meta AI built it, and it has become the standard in academia and is widely used in industry alongside TensorFlow. Understanding PyTorch at its foundation — not just calling high-level APIs — is what separates someone who can copy tutorials from someone who can debug models, design architectures, and understand what is actually happening during training.\n",
    "\n",
    "PyTorch has two fundamental components. First, a tensor library that works like NumPy but can run on GPUs. Second, an automatic differentiation engine called Autograd that tracks every operation you perform on tensors and can compute gradients of any quantity with respect to any other quantity automatically. Everything else in PyTorch — neural network layers, optimizers, loss functions — is built on these two primitives.\n",
    "\n",
    "If you have read the NumPy notebook, you will find the tensor API immediately familiar. The key differences are the device system, the default dtype (`float32` vs NumPy's `float64`), and the presence of `requires_grad`."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "install-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation\n",
    "\n",
    "PyTorch comes pre-installed on Google Colab. The cell below confirms the version and whether CUDA is available. If you are running this in Colab, go to Runtime > Change runtime type > T4 GPU to enable GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version :\", torch.__version__)\n",
    "print(\"CUDA available  :\", torch.cuda.is_available())\n",
    "print(\"MPS available   :\", torch.backends.mps.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name        :\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU memory (GB) :\", torch.cuda.get_device_properties(0).total_memory / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Device Setup\n",
    "\n",
    "Establishing the device once at the top of every script or notebook is standard practice. All tensors and models are moved to this device. Code written this way runs identically whether a GPU is available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Active device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device-setup-md2",
   "metadata": {},
   "source": [
    "This three-way check — CUDA (NVIDIA GPU), MPS (Apple Silicon), CPU — is the standard device selection pattern in modern PyTorch code. You will see it at the top of nearly every repository. Writing `device = \"cuda\" if torch.cuda.is_available() else \"cpu\"` is the older, simpler version. The pattern using `torch.device` is more explicit and supports string formatting like `f\"Using: {device}\"`."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "creation-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Tensors\n",
    "\n",
    "A tensor is PyTorch's fundamental data structure. It is conceptually identical to a NumPy ndarray with the addition of device and gradient tracking. The creation API mirrors NumPy almost exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "b = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "c = torch.tensor([[[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"1D:\", a)\n",
    "print(\"2D:\\n\", b)\n",
    "print(\"3D shape:\", c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-1-md",
   "metadata": {},
   "source": [
    "`torch.tensor` always copies the data and infers dtype from the Python or NumPy input. If the input is a Python list of integers, you get `int64`. If the input contains floats, you get `float32`. This is different from NumPy, which defaults to `float64` for floats. The dtype difference matters when you mix PyTorch and NumPy — converting a `float64` NumPy array directly to PyTorch gives a `float64` tensor, which most model layers will reject because they expect `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros   = torch.zeros(3, 4)\n",
    "ones    = torch.ones(2, 3)\n",
    "full    = torch.full((2, 2), fill_value=3.14)\n",
    "eye     = torch.eye(4)\n",
    "empty   = torch.empty(2, 3)\n",
    "\n",
    "print(\"zeros:\\n\", zeros)\n",
    "print(\"\\nones:\\n\", ones)\n",
    "print(\"\\nfull(3.14):\\n\", full)\n",
    "print(\"\\neye(4):\\n\", eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-2-md",
   "metadata": {},
   "source": [
    "Note that `torch.zeros`, `torch.ones`, and `torch.full` take the shape as separate arguments `(3, 4)` rather than a tuple — unlike NumPy's `np.zeros((3, 4))`. Both styles are accepted in PyTorch, but the unpacked form is more common in practice. `torch.empty` allocates memory without initializing it, which is faster when you know you will fill every element immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "uniform  = torch.rand(3, 4)\n",
    "normal   = torch.randn(3, 4)\n",
    "integers = torch.randint(low=0, high=10, size=(3, 4))\n",
    "perm     = torch.randperm(8)\n",
    "\n",
    "print(\"Uniform [0,1):\\n\", uniform)\n",
    "print(\"\\nStandard normal:\\n\", normal)\n",
    "print(\"\\nRandom integers:\\n\", integers)\n",
    "print(\"\\nRandom permutation:\", perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-3-md",
   "metadata": {},
   "source": [
    "`torch.manual_seed` sets the random seed for reproducibility. Setting it ensures that every time you run this code, `torch.rand` and `torch.randn` produce the same numbers. `torch.randperm` generates a random permutation of integers from 0 to n-1 — this is used directly in batch shuffling. In a data loader, you call `randperm(dataset_size)` and index into your data with the result to create a shuffled epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-like",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(\"zeros_like:\\n\", torch.zeros_like(x))\n",
    "print(\"ones_like:\\n\",  torch.ones_like(x))\n",
    "print(\"rand_like:\\n\",  torch.rand_like(x))\n",
    "print(\"empty_like shape:\", torch.empty_like(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-like-md",
   "metadata": {},
   "source": [
    "The `_like` family creates tensors with the same shape, dtype, and device as the input tensor. This is a critical pattern in model code. When you compute a mask, add Gaussian noise, or create a bias term, you want it on the same device as the existing tensors without explicitly specifying device every time. Using `torch.zeros_like(x)` instead of `torch.zeros(x.shape, device=x.device, dtype=x.dtype)` is cleaner and less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "arange = torch.arange(0, 10, 2)\n",
    "linsp  = torch.linspace(0.0, 1.0, 5)\n",
    "\n",
    "print(\"arange(0,10,2)  :\", arange)\n",
    "print(\"linspace(0,1,5) :\", linsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-range-md",
   "metadata": {},
   "source": [
    "`torch.arange` is the range equivalent. `torch.linspace` gives evenly spaced points from start to stop inclusive. These appear in positional encodings, learning rate schedulers, and plotting activation curves."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "attrs-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Tensor Attributes\n",
    "\n",
    "Every PyTorch tensor has four key attributes: shape, dtype, device, and requires_grad. The first three mirror NumPy. The fourth is unique to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attrs",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "\n",
    "print(\"shape         :\", x.shape)\n",
    "print(\"dtype         :\", x.dtype)\n",
    "print(\"device        :\", x.device)\n",
    "print(\"requires_grad :\", x.requires_grad)\n",
    "print(\"ndim          :\", x.ndim)\n",
    "print(\"numel         :\", x.numel())\n",
    "print(\"is_contiguous :\", x.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attrs-md2",
   "metadata": {},
   "source": [
    "`numel()` returns the total number of elements. `is_contiguous()` tells you whether the tensor's data is stored in a contiguous block of memory in C-order. Transposing a tensor makes it non-contiguous. Some operations require contiguous memory and will raise an error if the tensor is not — calling `.contiguous()` before `.view()` is a common fix.\n",
    "\n",
    "`requires_grad` is the switch that activates Autograd. Leaf tensors with `requires_grad=True` are the inputs to the computation graph — model parameters are exactly this. Setting this flag tells PyTorch: \"when gradients are computed, I need the gradient with respect to this tensor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dtype-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [\n",
    "    (torch.float16,   \"float16  (half precision)\"),\n",
    "    (torch.float32,   \"float32  (single — default)\"),\n",
    "    (torch.float64,   \"float64  (double)\"),\n",
    "    (torch.int8,      \"int8\"),\n",
    "    (torch.int32,     \"int32\"),\n",
    "    (torch.int64,     \"int64    (default int)\"),\n",
    "    (torch.bool,      \"bool\"),\n",
    "    (torch.bfloat16,  \"bfloat16 (used in TPU/modern GPUs)\"),\n",
    "]\n",
    "\n",
    "for dtype, label in dtypes:\n",
    "    t = torch.tensor([1.0], dtype=dtype)\n",
    "    print(f\"{label:40s} | itemsize: {t.element_size()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dtype-detail-md",
   "metadata": {},
   "source": [
    "dtype selection has real-world consequences. `float32` is the standard for model parameters and activations. `float16` halves memory and speeds up matrix multiplications on modern GPUs (A100, H100) but has a narrower range, risking overflow. `bfloat16` has the same range as `float32` but lower precision — it is the preferred half-precision format for training large language models. `int64` is used for class labels and token indices. Feeding an `int64` label tensor into a loss function that expects `float32` is a common error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "\n",
    "print(\"Original         :\", x.dtype)\n",
    "print(\".float()         :\", x.float().dtype)\n",
    "print(\".double()        :\", x.double().dtype)\n",
    "print(\".half()          :\", x.half().dtype)\n",
    "print(\".to(torch.float32):\", x.to(torch.float32).dtype)\n",
    "print(\".to(device)      : moves device + keeps dtype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casting-md",
   "metadata": {},
   "source": [
    "`.float()` is shorthand for `.to(torch.float32)`. `.to()` is the most flexible method — it accepts a dtype, a device, or both. You can write `x.to(device='cuda', dtype=torch.float16)` to move and cast in one call. In production model code, `.to(device)` is called on both the model and each batch of data at the start of the training loop."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "reshape-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Reshaping Tensors\n",
    "\n",
    "Reshaping in PyTorch works identically to NumPy in concept, with the distinction between `view` and `reshape` being important to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reshape",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(24)\n",
    "print(\"Original shape:\", a.shape)\n",
    "\n",
    "b = a.view(4, 6)\n",
    "c = a.reshape(2, 3, 4)\n",
    "d = a.view(-1, 6)\n",
    "e = a.reshape(4, -1)\n",
    "\n",
    "print(\"view(4, 6)    :\", b.shape)\n",
    "print(\"reshape(2,3,4):\", c.shape)\n",
    "print(\"view(-1, 6)   :\", d.shape)\n",
    "print(\"reshape(4, -1):\", e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reshape-md2",
   "metadata": {},
   "source": [
    "`view` requires the tensor to be contiguous in memory and always returns a view (shares data). `reshape` is more flexible — it returns a view when possible, and a copy when not. In practice, prefer `view` when you know the tensor is contiguous (freshly created, not transposed) for clarity of intent. If you hit a `RuntimeError: view size is not compatible`, call `.contiguous().view(...)`. The `-1` dimension inference works in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "squeeze-unsqueeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original        :\", x.shape)\n",
    "\n",
    "print(\"unsqueeze(0)    :\", x.unsqueeze(0).shape)\n",
    "print(\"unsqueeze(1)    :\", x.unsqueeze(1).shape)\n",
    "print(\"unsqueeze(-1)   :\", x.unsqueeze(-1).shape)\n",
    "print(\"[None, :]       :\", x[None, :].shape)\n",
    "print(\"[:, None]       :\", x[:, None].shape)\n",
    "\n",
    "y = torch.randn(1, 3, 1, 5)\n",
    "print(\"\\nBefore squeeze  :\", y.shape)\n",
    "print(\"squeeze()       :\", y.squeeze().shape)\n",
    "print(\"squeeze(0)      :\", y.squeeze(0).shape)\n",
    "print(\"squeeze(2)      :\", y.squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "squeeze-unsqueeze-md",
   "metadata": {},
   "source": [
    "`unsqueeze(dim)` inserts a size-1 dimension at the specified position. `None` indexing is an alias — `x[None, :]` is equivalent to `x.unsqueeze(0)`. You constantly use this to make shapes broadcastable. For example, an attention mask of shape `(batch, seq_len)` must be unsqueezed to `(batch, 1, 1, seq_len)` before being added to attention scores of shape `(batch, heads, seq_len, seq_len)`. `squeeze` removes size-1 dimensions — useful when a model outputs shape `(batch, 1)` for binary classification and you need `(batch,)` for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permute-transpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(\"Original shape          :\", x.shape)\n",
    "\n",
    "perm = x.permute(0, 2, 1)\n",
    "print(\"permute(0, 2, 1) shape  :\", perm.shape)\n",
    "\n",
    "m = torch.randn(3, 4)\n",
    "print(\"\\nMatrix shape            :\", m.shape)\n",
    "print(\".T shape                :\", m.T.shape)\n",
    "print(\"transpose(0,1) shape    :\", m.transpose(0, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permute-transpose-md",
   "metadata": {},
   "source": [
    "`permute` reorders all dimensions in one call by specifying the desired order. `.T` and `transpose(dim0, dim1)` swap two specific dimensions. In image processing, data often arrives as `(batch, height, width, channels)` (NHWC format) and PyTorch convolutions expect `(batch, channels, height, width)` (NCHW). The conversion is `x.permute(0, 3, 1, 2)`. In Transformer attention, the query, key, and value tensors are permuted multiple times to align dimensions for batched matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten-view",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features = torch.randn(32, 8, 8, 64)\n",
    "print(\"Input shape (batch, H, W, C):\", batch_features.shape)\n",
    "\n",
    "flat = batch_features.flatten(start_dim=1)\n",
    "print(\"After flatten(start_dim=1)  :\", flat.shape)\n",
    "\n",
    "flat2 = batch_features.view(32, -1)\n",
    "print(\"After view(32, -1)          :\", flat2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten-view-md",
   "metadata": {},
   "source": [
    "This is the canonical \"flatten before a linear layer\" operation. A convolutional network produces a 4D tensor. Before the classification head (a linear layer), you collapse all non-batch dimensions into one. `flatten(start_dim=1)` leaves the batch dimension untouched and flattens everything else. This is equivalent to `view(batch_size, -1)`. The resulting shape `(32, 4096)` is then fed into a `nn.Linear(4096, num_classes)` layer."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "indexing-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Indexing and Slicing\n",
    "\n",
    "PyTorch indexing is essentially identical to NumPy. The same slice notation, boolean masking, and fancy indexing all work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indexing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[10, 20, 30, 40],\n",
    "                  [50, 60, 70, 80],\n",
    "                  [90,100,110,120]])\n",
    "\n",
    "print(\"x[0, 0]    =\", x[0, 0])\n",
    "print(\"x[1, 2]    =\", x[1, 2])\n",
    "print(\"x[-1, -1]  =\", x[-1, -1])\n",
    "print(\"Scalar val :\", x[0, 0].item())\n",
    "\n",
    "print(\"\\nRow 0              :\", x[0])\n",
    "print(\"Column 1           :\", x[:, 1])\n",
    "print(\"First two rows     :\\n\", x[:2])\n",
    "print(\"Submatrix [0:2,1:3]:\\n\", x[0:2, 1:3])\n",
    "print(\"Every other col    :\", x[:, ::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indexing-md2",
   "metadata": {},
   "source": [
    "The key difference from NumPy: accessing a single element gives a 0-dimensional tensor, not a Python scalar. Call `.item()` to extract a Python scalar — this is essential when logging loss values. `loss.item()` is the correct way to get the numeric value without keeping the tensor in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bool-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.0, 7.0, -1.0, 12.0, -5.0, 9.0])\n",
    "\n",
    "mask = x > 0\n",
    "print(\"Mask        :\", mask)\n",
    "print(\"Positive    :\", x[mask])\n",
    "\n",
    "y = x.clone()\n",
    "y[y < 0] = 0.0\n",
    "print(\"ReLU manual :\", y)\n",
    "\n",
    "result = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "print(\"torch.where :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bool-index-md",
   "metadata": {},
   "source": [
    "`torch.where(condition, x, y)` selects elements from `x` where condition is True and from `y` where it is False. This is the vectorized ternary operator and it supports autograd — `y[y < 0] = 0` does not. When implementing custom activation functions that need gradients, use `torch.where` or torch functional equivalents, not in-place boolean assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gather-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[0.1, 2.5, -0.3],\n",
    "                       [1.2, 0.5, 3.1],\n",
    "                       [0.8, 1.9, 0.2]])\n",
    "\n",
    "labels = torch.tensor([1, 2, 1])\n",
    "\n",
    "gathered = logits.gather(dim=1, index=labels.unsqueeze(1))\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"Labels :\", labels)\n",
    "print(\"Gathered (correct class logits):\", gathered.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gather-scatter-md",
   "metadata": {},
   "source": [
    "`gather` picks elements along a dimension using an index tensor. Here it collects the logit corresponding to the correct class for each sample in the batch. This operation appears inside cross-entropy loss implementations and in reinforcement learning when you need to select Q-values for the actions that were actually taken. It is fully differentiable."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "math-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Math Operations\n",
    "\n",
    "PyTorch math mirrors NumPy's element-wise design. All standard operators are overloaded and every operation has a functional equivalent in `torch.*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elem-math",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "b = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "\n",
    "print(\"a + b  :\", a + b)\n",
    "print(\"a - b  :\", a - b)\n",
    "print(\"a * b  :\", a * b)\n",
    "print(\"a / b  :\", a / b)\n",
    "print(\"a ** 2 :\", a ** 2)\n",
    "print(\"a % 3  :\", a % 3)\n",
    "\n",
    "print(\"\\ntorch.sin  :\", torch.sin(a))\n",
    "print(\"torch.exp  :\", torch.exp(a))\n",
    "print(\"torch.log  :\", torch.log(a))\n",
    "print(\"torch.sqrt :\", torch.sqrt(a))\n",
    "print(\"torch.abs  :\", torch.abs(torch.tensor([-3.0, -1.0, 2.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elem-math-md",
   "metadata": {},
   "source": [
    "All of these operations are tracked by Autograd when the input tensors have `requires_grad=True`. Every arithmetic operation adds a node to the computation graph. The graph is built dynamically — this is what PyTorch calls define-by-run (dynamic computation graph), as opposed to TensorFlow 1.x's define-then-run approach. You can use normal Python control flow (if statements, for loops) and the graph is built differently each forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matmul",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "B = torch.tensor([[7.0, 8.0],\n",
    "                  [9.0, 10.0],\n",
    "                  [11.0, 12.0]])\n",
    "\n",
    "C = A @ B\n",
    "print(\"A shape:\", A.shape, \" B shape:\", B.shape)\n",
    "print(\"A @ B shape:\", C.shape)\n",
    "print(\"A @ B:\\n\", C)\n",
    "\n",
    "u = torch.tensor([1.0, 2.0, 3.0])\n",
    "v = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(\"\\nDot product        :\", torch.dot(u, v))\n",
    "\n",
    "A_batch = torch.randn(8, 3, 4)\n",
    "B_batch = torch.randn(8, 4, 5)\n",
    "C_batch = torch.bmm(A_batch, B_batch)\n",
    "print(\"\\nBatched matmul (bmm) shape:\", C_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matmul-md",
   "metadata": {},
   "source": [
    "`@` and `torch.matmul` handle both 2D matrix multiplication and batched variants for higher-dimensional tensors. `torch.bmm` (batch matrix multiply) is the explicit 3D-only version. In Transformer attention, `scores = queries @ keys.transpose(-2, -1)` is the scaled dot-product attention computation, operating on tensors of shape `(batch, heads, seq_len, head_dim)`. The `@` operator broadcasts correctly over the batch and head dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0],\n",
    "                  [7.0, 8.0, 9.0]])\n",
    "\n",
    "print(\"sum all    :\", x.sum())\n",
    "print(\"sum dim=0  :\", x.sum(dim=0))\n",
    "print(\"sum dim=1  :\", x.sum(dim=1))\n",
    "print(\"mean all   :\", x.mean())\n",
    "print(\"max dim=1  :\", x.max(dim=1))\n",
    "print(\"argmax dim=1:\",x.argmax(dim=1))\n",
    "print(\"std dim=0  :\", x.std(dim=0))\n",
    "print(\"norm       :\", x.norm())\n",
    "print(\"norm dim=1 :\", x.norm(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregation-md",
   "metadata": {},
   "source": [
    "In PyTorch the keyword is `dim=` everywhere. This replaces NumPy's `axis=`. `x.max(dim=1)` returns a named tuple with `.values` and `.indices`. `x.argmax(dim=1)` is the direct equivalent of NumPy's argmax and is used to convert batch logit tensors to predicted class indices for accuracy computation. `x.norm(dim=1)` computes the L2 norm of each row — this is used in gradient clipping and embedding normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "y = x + 10\n",
    "print(\"Out-of-place (x + 10):\", y, \"| x unchanged:\", x)\n",
    "\n",
    "x.add_(10)\n",
    "print(\"In-place (add_)      :\", x)\n",
    "\n",
    "x.mul_(2)\n",
    "print(\"In-place (mul_)      :\", x)\n",
    "\n",
    "x.zero_()\n",
    "print(\"In-place (zero_)     :\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inplace-md",
   "metadata": {},
   "source": [
    "In-place operations use a trailing underscore (`add_`, `mul_`, `zero_`). They modify the tensor without allocating new memory. This is faster and reduces memory usage, but there is a critical constraint: **never use in-place operations on tensors that require gradients or are part of the computation graph**. PyTorch records operations for backpropagation, and in-place modification destroys the information needed to compute gradients. The one legitimate use of in-place ops is `optimizer.zero_grad()` which calls `.zero_()` internally on parameter gradients."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "autograd-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Autograd: Automatic Differentiation\n",
    "\n",
    "Autograd is the engine that makes training neural networks possible. Instead of manually deriving and coding gradient formulas, you define the forward computation and PyTorch automatically computes all gradients via backpropagation.\n",
    "\n",
    "The mechanism: PyTorch builds a directed acyclic graph (DAG) as you perform operations on tensors with `requires_grad=True`. Each node stores the operation and a function to compute its local gradient. When you call `.backward()`, PyTorch traverses this graph in reverse (reverse-mode automatic differentiation, also called backpropagation) applying the chain rule at every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autograd-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "y = x ** 2 + 3 * x + 5\n",
    "\n",
    "print(\"x        :\", x)\n",
    "print(\"y = x^2 + 3x + 5 :\", y)\n",
    "print(\"grad_fn  :\", y.grad_fn)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"\\ndy/dx at x=4     :\", x.grad.item())\n",
    "print(\"Expected (2x+3)  :\", (2*4 + 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autograd-basic-md",
   "metadata": {},
   "source": [
    "`grad_fn` shows the last operation in the graph — here `AddBackward0` because the last operation was an addition. After `backward()`, `x.grad` holds `dy/dx`. PyTorch computed this using the chain rule: `d/dx(x^2 + 3x + 5) = 2x + 3`. At `x=4`, that is `2*4+3 = 11`. This is exactly what gradient descent uses: it updates `x` by subtracting a fraction of this gradient to minimize `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autograd-multi",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "z = (a + b) * c\n",
    "\n",
    "print(\"z = (a+b)*c =\", z.item())\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"\\ndz/da :\", a.grad.item(), \"  (expected:\", c.item(), \")\")\n",
    "print(\"dz/db :\", b.grad.item(), \"  (expected:\", c.item(), \")\")\n",
    "print(\"dz/dc :\", c.grad.item(), \"  (expected:\", (a+b).item(), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autograd-multi-md",
   "metadata": {},
   "source": [
    "All leaf tensors with `requires_grad=True` receive gradients simultaneously from a single `backward()` call. PyTorch traverses the graph once and distributes gradients everywhere. In a neural network with millions of parameters, this single traversal computes gradients for every parameter at once. The chain rule for `z = (a+b)*c` gives `dz/dc = a+b = 5` (the derivative of multiplication with respect to the multiplier is the other factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autograd-accum",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "for i in range(4):\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(f\"Pass {i+1}: x.grad = {x.grad.item()}\")\n",
    "\n",
    "print(\"\\n--- With zero_grad() ---\")\n",
    "x.grad.zero_()\n",
    "\n",
    "for i in range(4):\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(f\"Pass {i+1}: x.grad = {x.grad.item()}\")\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autograd-accum-md",
   "metadata": {},
   "source": [
    "Gradients accumulate by default. Each `backward()` call adds to `x.grad` rather than replacing it. This is a deliberate design for cases where you want to sum gradients across multiple forward passes before updating (gradient accumulation, used when GPU memory is too small for the desired batch size). In normal training, you must zero gradients before each backward pass. `optimizer.zero_grad()` does this automatically for all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "no-grad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "W = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = W @ x\n",
    "    print(\"Inside no_grad:\")\n",
    "    print(\"  y.requires_grad:\", y.requires_grad)\n",
    "    print(\"  y.grad_fn      :\", y.grad_fn)\n",
    "\n",
    "y_track = W @ x\n",
    "print(\"\\nOutside no_grad:\")\n",
    "print(\"  y.requires_grad:\", y_track.requires_grad)\n",
    "print(\"  y.grad_fn      :\", y_track.grad_fn)\n",
    "\n",
    "detached = x.detach()\n",
    "print(\"\\nDetached tensor requires_grad:\", detached.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "no-grad-md",
   "metadata": {},
   "source": [
    "`torch.no_grad()` disables gradient tracking for all operations inside the block. No computation graph is built, which reduces memory usage significantly. Use this during evaluation and inference — there is no point tracking gradients when you will not call `backward()`. `.detach()` creates a new tensor that shares data with the original but is detached from the computation graph. This is used when you want to use a tensor's value as a constant in further computation, or when logging/visualizing intermediate values."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "gpu-md",
   "metadata": {},
   "source": [
    "---\n",
    "## GPU Operations\n",
    "\n",
    "Moving computation to a GPU can provide 10x to 100x speedup for the matrix multiplications that dominate deep learning. PyTorch makes this transparent — the same code runs on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-move",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = torch.randn(3, 4)\n",
    "print(\"CPU tensor device:\", x_cpu.device)\n",
    "\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(\"GPU tensor device:\", x_gpu.device)\n",
    "\n",
    "y_gpu = torch.randn(3, 4, device=device)\n",
    "print(\"Created on device:\", y_gpu.device)\n",
    "\n",
    "z = x_gpu + y_gpu\n",
    "print(\"Result device    :\", z.device)\n",
    "\n",
    "z_cpu = z.cpu()\n",
    "print(\"Back to CPU      :\", z_cpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-move-md",
   "metadata": {},
   "source": [
    "Tensors on different devices cannot interact. Attempting `cpu_tensor + gpu_tensor` raises a RuntimeError. All operands must be on the same device. In training loops, you move the model to the device once before training, then move each batch to the device at the start of each iteration. The pattern is `X, y = X.to(device), y.to(device)` inside the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "size = 2048\n",
    "\n",
    "A_cpu = torch.randn(size, size)\n",
    "B_cpu = torch.randn(size, size)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(5):\n",
    "    C = A_cpu @ B_cpu\n",
    "cpu_time = (time.time() - start) / 5\n",
    "print(f\"CPU matmul {size}x{size}: {cpu_time:.4f}s per call\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    A_gpu = A_cpu.to(device)\n",
    "    B_gpu = B_cpu.to(device)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(5):\n",
    "        C = A_gpu @ B_gpu\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = (time.time() - start) / 5\n",
    "    print(f\"GPU matmul {size}x{size}: {gpu_time:.4f}s per call\")\n",
    "    print(f\"Speedup: {cpu_time / gpu_time:.1f}x\")\n",
    "else:\n",
    "    print(\"No GPU available — enable Colab GPU runtime to see the speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-benchmark-md",
   "metadata": {},
   "source": [
    "`torch.cuda.synchronize()` is essential for accurate GPU benchmarking. GPU operations are asynchronous — the CPU issues a command and moves on without waiting for the GPU to finish. Without synchronize, you are timing only the CPU dispatch time, not the actual computation. The speedup for large matrix multiplications on a T4 GPU is typically 20-50x over CPU."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "stacking-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Concatenation and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "b = torch.tensor([[4.0, 5.0, 6.0]])\n",
    "\n",
    "cat_0 = torch.cat([a, b], dim=0)\n",
    "cat_1 = torch.cat([a, b], dim=1)\n",
    "stk_0 = torch.stack([a, b], dim=0)\n",
    "\n",
    "print(\"cat dim=0 shape:\", cat_0.shape, \"\\n\", cat_0)\n",
    "print(\"cat dim=1 shape:\", cat_1.shape, \"\\n\", cat_1)\n",
    "print(\"stack dim=0 shape:\", stk_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stacking-md2",
   "metadata": {},
   "source": [
    "`torch.cat` joins tensors along an existing dimension. `torch.stack` creates a new dimension. In training, you typically collect model outputs from multiple mini-batches and concatenate them for evaluation: `all_preds = torch.cat(pred_list, dim=0)`. When assembling a batch from individual samples, you stack them: `batch = torch.stack([sample1, sample2, ...], dim=0)`."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "numpy-interop-md",
   "metadata": {},
   "source": [
    "---\n",
    "## NumPy Interoperability\n",
    "\n",
    "PyTorch and NumPy are designed to work together. Conversions are essentially free on CPU because they share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interop",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = np.array([[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0]])\n",
    "\n",
    "t_shared = torch.from_numpy(np_arr)\n",
    "t_copy   = torch.tensor(np_arr)\n",
    "\n",
    "print(\"NumPy dtype  :\", np_arr.dtype)\n",
    "print(\"Tensor dtype :\", t_shared.dtype)\n",
    "\n",
    "np_arr[0, 0] = 999.0\n",
    "print(\"\\nAfter modifying np_arr[0,0]:\")\n",
    "print(\"t_shared[0,0] :\", t_shared[0, 0].item(), \"  (shared — changed)\")\n",
    "print(\"t_copy[0,0]   :\", t_copy[0, 0].item(), \"   (copy — unchanged)\")\n",
    "\n",
    "t = torch.randn(2, 3)\n",
    "arr = t.numpy()\n",
    "print(\"\\nTensor to NumPy:\", type(arr), arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interop-md",
   "metadata": {},
   "source": [
    "`torch.from_numpy` creates a tensor sharing the same memory as the NumPy array. No data is copied. Modifying either one modifies both. `torch.tensor` always copies. For GPU tensors, you must call `.cpu()` first before converting to NumPy. The full pattern for going from a GPU tensor to a NumPy array that may have gradients is: `tensor.detach().cpu().numpy()`. This is the standard way to convert model outputs to NumPy for evaluation metrics in Scikit-learn."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "full-example-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Putting It Together: Manual Training Loop from Scratch\n",
    "\n",
    "This builds a complete training loop using nothing but tensors and Autograd — no `nn.Module`, no optimizer class, no loss class. Seeing it at this level makes the high-level APIs less magical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples  = 200\n",
    "n_features = 4\n",
    "n_classes  = 3\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "true_W = torch.randn(n_features, n_classes)\n",
    "logits_true = X @ true_W\n",
    "y = logits_true.argmax(dim=1)\n",
    "\n",
    "W = torch.randn(n_features, n_classes, requires_grad=True)\n",
    "b = torch.zeros(n_classes, requires_grad=True)\n",
    "\n",
    "lr     = 0.05\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = X @ W + b\n",
    "\n",
    "    exp_logits  = torch.exp(logits - logits.max(dim=1, keepdim=True).values)\n",
    "    probs       = exp_logits / exp_logits.sum(dim=1, keepdim=True)\n",
    "    correct_log = torch.log(probs[torch.arange(n_samples), y])\n",
    "    loss        = -correct_log.mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            preds    = (X @ W + b).argmax(dim=1)\n",
    "            accuracy = (preds == y).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-loop-md",
   "metadata": {},
   "source": [
    "Several important patterns appear here. The numerically stable softmax subtracts the row maximum before exponentiation — without this, large logit values cause `exp` to overflow to infinity. The cross-entropy loss is the negative log probability assigned to the correct class. The weight update uses `torch.no_grad()` because the update step itself should not be tracked by Autograd — you are modifying parameters, not computing gradients. `zero_()` clears gradients after the update so they do not accumulate into the next iteration.\n",
    "\n",
    "Every modern deep learning framework wraps these exact steps: `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-api-md",
   "metadata": {},
   "source": [
    "---\n",
    "## The Same Loop with Modern PyTorch APIs\n",
    "\n",
    "Now the identical problem using `nn.Linear`, `nn.CrossEntropyLoss`, and `torch.optim.Adam`. Compare this to the manual version above to see what each API is abstracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X = torch.randn(200, 4)\n",
    "true_W = torch.randn(4, 3)\n",
    "y = (X @ true_W).argmax(dim=1)\n",
    "\n",
    "model     = nn.Linear(in_features=4, out_features=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "model.to(device)\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(X)\n",
    "    loss   = criterion(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            preds    = logits.argmax(dim=1)\n",
    "            accuracy = (preds == y).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-loop-md",
   "metadata": {},
   "source": [
    "`nn.Linear` initializes weights with Kaiming uniform initialization (a carefully chosen distribution that keeps activations from vanishing or exploding). `nn.CrossEntropyLoss` combines the numerically stable softmax and the negative log-likelihood into a single operation. `torch.optim.Adam` implements adaptive moment estimation — it maintains a running mean and variance of gradients to scale each parameter's update individually, which makes it much faster to converge than plain gradient descent.\n",
    "\n",
    "The structure `zero_grad → forward → loss → backward → step` is the canonical PyTorch training loop. You will find this exact structure in every repository, from a small classifier to GPT-scale language model training."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Reference: NumPy vs PyTorch\n",
    "\n",
    "| Concept | NumPy | PyTorch | Notes |\n",
    "|---|---|---|---|\n",
    "| Create from list | `np.array([1,2,3])` | `torch.tensor([1,2,3])` | |\n",
    "| Zeros | `np.zeros((m,n))` | `torch.zeros(m, n)` | PyTorch: no outer tuple |\n",
    "| Random uniform | `np.random.rand(m,n)` | `torch.rand(m, n)` | |\n",
    "| Random normal | `np.random.randn(m,n)` | `torch.randn(m, n)` | |\n",
    "| Range | `np.arange(a,b,s)` | `torch.arange(a,b,s)` | |\n",
    "| Default float dtype | `float64` | `float32` | Important difference |\n",
    "| Shape | `a.shape` | `a.shape` | Same |\n",
    "| Dtype | `a.dtype` | `a.dtype` | |\n",
    "| Type cast | `a.astype(np.float32)` | `a.float()` / `.to(torch.float32)` | |\n",
    "| Reshape | `a.reshape(m,n)` | `a.reshape(m,n)` / `a.view(m,n)` | view requires contiguous |\n",
    "| Transpose | `a.T` | `a.T` / `a.transpose(d0,d1)` | |\n",
    "| Permute dims | `np.transpose(a, axes)` | `a.permute(d0,d1,d2)` | |\n",
    "| Add dim | `np.expand_dims(a,0)` | `a.unsqueeze(0)` | |\n",
    "| Remove dim | `np.squeeze(a)` | `a.squeeze()` | |\n",
    "| Flatten | `a.flatten()` | `a.flatten()` | |\n",
    "| Reduction axis | `a.sum(axis=0)` | `a.sum(dim=0)` | keyword differs |\n",
    "| Matrix multiply | `A @ B` | `A @ B` / `torch.mm(A,B)` | |\n",
    "| Batched matmul | — | `torch.bmm(A,B)` | 3D only |\n",
    "| Element scalar | `a[0, 0]` → Python scalar | `a[0, 0].item()` → Python scalar | .item() needed |\n",
    "| Copy | `a.copy()` | `a.clone()` | |\n",
    "| Where | `np.where(c, x, y)` | `torch.where(c, x, y)` | |\n",
    "| Concatenate | `np.concatenate([a,b], axis=0)` | `torch.cat([a,b], dim=0)` | |\n",
    "| Stack | `np.stack([a,b], axis=0)` | `torch.stack([a,b], dim=0)` | |\n",
    "| To/from NumPy | — | `torch.from_numpy(arr)` / `t.numpy()` | Shares memory on CPU |\n",
    "| GPU move | — | `.to(device)` / `.cuda()` | |\n",
    "| Track gradients | — | `requires_grad=True` | PyTorch only |\n",
    "| Disable grad | — | `torch.no_grad()` | Use for inference |\n",
    "| Compute grads | — | `loss.backward()` | Fills `.grad` attributes |\n",
    "| Clear grads | — | `optimizer.zero_grad()` | Must call each iteration |\n",
    "| Seed | `np.random.seed(n)` | `torch.manual_seed(n)` | |"
   ]
  }

 ]
}
