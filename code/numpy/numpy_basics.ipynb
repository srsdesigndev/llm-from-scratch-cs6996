{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [

  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# NumPy — Complete Foundations\n",
    "\n",
    "NumPy (Numerical Python) is the backbone of scientific computing in Python. Every major library you will encounter — Pandas, Matplotlib, Scikit-learn, TensorFlow, PyTorch — either wraps NumPy or heavily borrows its design. Before you can understand machine learning, data science, or deep learning, you must understand NumPy.\n",
    "\n",
    "A Python list is flexible but slow. It can hold mixed types, which means Python cannot predict how much memory each element needs. NumPy solves this with the `ndarray` — an N-dimensional array where every element has the same type, stored in one contiguous block of memory. Operations run in C under the hood, making NumPy tens to hundreds of times faster than a Python loop.\n",
    "\n",
    "This notebook walks you from installation through every core concept with explanations tied to how things are actually used in modern machine learning workflows."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "install-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation\n",
    "\n",
    "NumPy comes pre-installed on Google Colab. The cell below upgrades it to the latest version and confirms the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Why NumPy Exists\n",
    "\n",
    "Before seeing the benchmarks, it is worth understanding why the speed gap exists. A Python list stores pointers to objects scattered in memory. When you add two lists element-by-element, Python must follow each pointer, unbox the integer, do the math, and box the result back. NumPy stores raw numbers in a flat block of memory and hands the entire array to compiled C code. No boxing, no pointer chasing, no Python overhead per element.\n",
    "\n",
    "This matters enormously in machine learning where you routinely operate on matrices with millions of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "why-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n = 1_000_000\n",
    "py_list = list(range(n))\n",
    "np_arr  = np.arange(n)\n",
    "\n",
    "start = time.time()\n",
    "result = [x * 2 for x in py_list]\n",
    "python_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result = np_arr * 2\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"Python list : {python_time:.4f} seconds\")\n",
    "print(f\"NumPy array : {numpy_time:.4f} seconds\")\n",
    "print(f\"Speedup     : {python_time / numpy_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-bench-md",
   "metadata": {},
   "source": [
    "That speedup is not a trick. It is the direct result of memory layout and compiled operations. Every time a neural network multiplies weights by activations, or when Pandas computes a column mean, this is what is happening underneath."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "creation-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Arrays\n",
    "\n",
    "There are three categories of array creation: from existing Python data, from built-in generators, and from file or other arrays. You will use all three regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "c = np.array([[[1, 2], [3, 4]],\n",
    "              [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"1D array  :\", a)\n",
    "print(\"2D array  :\\n\", b)\n",
    "print(\"3D shape  :\", c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-1-md",
   "metadata": {},
   "source": [
    "A 1D array maps to a vector, a 2D array maps to a matrix, and a 3D array maps to a batch of matrices. In deep learning, image data is typically stored as a 4D array of shape `(batch_size, channels, height, width)`. The nesting depth of your Python list determines the number of dimensions NumPy creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros   = np.zeros((3, 4))\n",
    "ones    = np.ones((2, 3))\n",
    "full    = np.full((3, 3), fill_value=7.0)\n",
    "eye     = np.eye(4)\n",
    "empty   = np.empty((2, 2))\n",
    "\n",
    "print(\"zeros:\\n\", zeros)\n",
    "print(\"\\nones:\\n\", ones)\n",
    "print(\"\\nfull(7.0):\\n\", full)\n",
    "print(\"\\neye(4):\\n\", eye)\n",
    "print(\"\\nempty (uninitialized values):\\n\", empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-2-md",
   "metadata": {},
   "source": [
    "`np.zeros` and `np.ones` are used constantly when initializing weight matrices or bias vectors before training. `np.eye` creates an identity matrix, which appears in linear algebra proofs and certain neural network weight initializations. `np.empty` allocates memory without initializing it — it is faster when you know you will fill every element immediately anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arange  = np.arange(0, 10, 2)\n",
    "linsp   = np.linspace(0, 1, 9)\n",
    "logsp   = np.logspace(0, 3, 4)\n",
    "\n",
    "print(\"arange(0, 10, 2) :\", arange)\n",
    "print(\"linspace(0, 1, 9):\", linsp)\n",
    "print(\"logspace(0, 3, 4):\", logsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-3-md",
   "metadata": {},
   "source": [
    "`np.arange` works like Python's built-in `range` but returns an array and accepts floats. `np.linspace` is different in a subtle but important way: you specify how many points you want, not the step size, and the endpoint is always included. This is the standard tool for creating evenly spaced evaluation grids when plotting functions. `np.logspace` spaces points logarithmically — essential when sweeping hyperparameters like learning rate over several orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creation-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "uniform  = rng.random((3, 3))\n",
    "normal   = rng.standard_normal((3, 3))\n",
    "integers = rng.integers(0, 10, size=(3, 3))\n",
    "\n",
    "print(\"Uniform [0,1):\\n\", uniform)\n",
    "print(\"\\nStandard normal:\\n\", normal)\n",
    "print(\"\\nRandom integers [0,10):\\n\", integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creation-4-md",
   "metadata": {},
   "source": [
    "The `default_rng` generator is the modern NumPy approach to random number generation. Setting a seed guarantees the same random values every run, which is critical for reproducibility in experiments. In machine learning, reproducibility means another researcher can run your code and get exactly the same results. The standard normal distribution (mean 0, standard deviation 1) is specifically used in weight initialization schemes like Xavier and He initialization, which directly affect whether a neural network trains successfully."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "attrs-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Array Attributes: Shape, Dtype, Ndim\n",
    "\n",
    "These three attributes describe everything structurally important about an array. Misunderstanding them is the source of most shape errors in machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attrs",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.0, 2.0, 3.0],\n",
    "              [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"Array:\\n\", x)\n",
    "print(\"shape :\", x.shape)    \n",
    "print(\"ndim  :\", x.ndim)     \n",
    "print(\"dtype :\", x.dtype)    \n",
    "print(\"size  :\", x.size)     \n",
    "print(\"itemsize:\", x.itemsize, \"bytes\") \n",
    "print(\"nbytes  :\", x.nbytes, \"bytes\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attrs-md2",
   "metadata": {},
   "source": [
    "Shape `(2, 3)` means 2 rows and 3 columns. The shape is always a tuple, and its length equals `ndim`. In neural networks, you constantly check shapes to verify that matrix multiplications are compatible — two matrices can multiply only when the inner dimensions match: `(A, B) @ (B, C)` gives `(A, C)`.\n",
    "\n",
    "The dtype determines memory use and precision. `float64` uses 8 bytes per element; `float32` uses 4. Halving precision roughly doubles how much data fits in GPU memory, which is why most deep learning uses `float32` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dtype-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_arr   = np.array([1, 2, 3], dtype=np.int8)\n",
    "int32_arr  = np.array([1, 2, 3], dtype=np.int32)\n",
    "float32    = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n",
    "float64    = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "complex128 = np.array([1+2j, 3+4j], dtype=np.complex128)\n",
    "bool_arr   = np.array([True, False, True], dtype=bool)\n",
    "\n",
    "for arr, name in [(int8_arr,'int8'), (int32_arr,'int32'), (float32,'float32'),\n",
    "                  (float64,'float64'), (bool_arr,'bool')]:\n",
    "    print(f\"{name:10s} | dtype: {str(arr.dtype):10s} | itemsize: {arr.itemsize} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dtype-detail-md",
   "metadata": {},
   "source": [
    "Choosing the right dtype is a practical skill. Model weights are stored in `float32`. Integer labels for classification (class 0, class 1, ...) are stored in `int64`. Boolean masks are stored as `bool`. Using the wrong dtype triggers silent type promotion or explicit errors. When you load image pixels from disk (values 0–255), they arrive as `uint8`. You must cast them to `float32` and normalize them before feeding them into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = np.array([0, 128, 255], dtype=np.uint8)\n",
    "print(\"Raw pixels       :\", pixels, \"|\", pixels.dtype)\n",
    "\n",
    "normalized = pixels.astype(np.float32) / 255.0\n",
    "print(\"Normalized float :\", normalized, \"|\", normalized.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casting-md",
   "metadata": {},
   "source": [
    "This pattern — load as `uint8`, cast to `float32`, divide by 255 — is something you will write or see written thousands of times when working with image data. The division brings values into the `[0.0, 1.0]` range that neural networks train on efficiently."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "reshape-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Reshaping, Views, and Copies\n",
    "\n",
    "Reshaping is about reinterpreting the same data with a different shape. The total number of elements must stay the same. Understanding the difference between a view and a copy prevents subtle bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reshape",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(24)\n",
    "print(\"Original shape:\", a.shape)\n",
    "\n",
    "b = a.reshape(4, 6)\n",
    "c = a.reshape(2, 3, 4)\n",
    "d = a.reshape(-1, 6)\n",
    "e = a.reshape(4, -1)\n",
    "\n",
    "print(\"reshape(4, 6)  :\", b.shape)\n",
    "print(\"reshape(2,3,4) :\", c.shape)\n",
    "print(\"reshape(-1, 6) :\", d.shape)\n",
    "print(\"reshape(4, -1) :\", e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reshape-md2",
   "metadata": {},
   "source": [
    "The `-1` in reshape is a convenience: it tells NumPy to compute that dimension automatically. This is used constantly in real code because you often know one dimension (say, batch size) but want NumPy to handle the rest. For example, after a convolutional layer outputs a 3D feature map, you reshape it to `(-1, features)` to feed into a linear layer without hardcoding the exact size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-copy",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "view = original.reshape(2, 3)\n",
    "copy = original.reshape(2, 3).copy()\n",
    "\n",
    "view[0, 0] = 999\n",
    "print(\"After modifying view[0,0]:\")\n",
    "print(\"original :\", original)\n",
    "print(\"view     :\\n\", view)\n",
    "\n",
    "copy[0, 0] = 777\n",
    "print(\"\\nAfter modifying copy[0,0]:\")\n",
    "print(\"original :\", original)\n",
    "print(\"copy     :\\n\", copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-copy-md",
   "metadata": {},
   "source": [
    "A view shares the underlying memory with the original array. No data is moved or copied — only the shape metadata changes. Modifying a view modifies the original. This is efficient but dangerous if you do not expect it. A copy owns its own memory. Changes to a copy never affect the original.\n",
    "\n",
    "You can check whether an array owns its data with `a.base is None`. If it is `None`, the array owns its data. If it points to another array, it is a view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten-transpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(\"Original shape:\", m.shape)\n",
    "print(\"Flattened     :\", m.flatten())\n",
    "print(\"Ravel         :\", m.ravel())\n",
    "print(\"Transposed    :\\n\", m.T)\n",
    "print(\"Transposed shape:\", m.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten-transpose-md",
   "metadata": {},
   "source": [
    "`flatten()` always returns a copy. `ravel()` returns a view when possible, making it faster. The transpose `.T` swaps all axes. For a 2D matrix, this turns rows into columns. For higher-dimensional arrays, use `np.transpose(arr, axes)` to specify exactly which axes to reorder — this is the NumPy equivalent of PyTorch's `permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expand-squeeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 2.0, 3.0])\n",
    "print(\"Original shape          :\", x.shape)\n",
    "\n",
    "row_vector = np.expand_dims(x, axis=0)\n",
    "col_vector = np.expand_dims(x, axis=1)\n",
    "print(\"expand_dims(axis=0) :\", row_vector.shape)\n",
    "print(\"expand_dims(axis=1) :\", col_vector.shape)\n",
    "\n",
    "y = np.array([[[1, 2, 3]]])\n",
    "print(\"\\nBefore squeeze:\", y.shape)\n",
    "print(\"After squeeze() :\", np.squeeze(y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expand-squeeze-md",
   "metadata": {},
   "source": [
    "`expand_dims` adds a size-1 dimension at the specified axis. This is needed constantly when broadcasting — for instance, adding a bias vector of shape `(features,)` to a batch of outputs of shape `(batch, features)` requires the bias to behave as shape `(1, features)`. `squeeze` removes all size-1 dimensions, which is the reverse operation. Both appear heavily in data pipeline code."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "indexing-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Indexing and Slicing\n",
    "\n",
    "NumPy's indexing is a generalization of Python list indexing to multiple dimensions. Mastering it eliminates the need for most explicit loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[10, 20, 30, 40],\n",
    "              [50, 60, 70, 80],\n",
    "              [90,100,110,120]])\n",
    "\n",
    "print(\"a[0, 0]   =\", a[0, 0])\n",
    "print(\"a[1, 2]   =\", a[1, 2])\n",
    "print(\"a[-1, -1] =\", a[-1, -1])\n",
    "\n",
    "print(\"\\nRow 0              :\", a[0])\n",
    "print(\"Column 1           :\", a[:, 1])\n",
    "print(\"First two rows     :\\n\", a[:2])\n",
    "print(\"Submatrix [0:2,1:3]:\\n\", a[0:2, 1:3])\n",
    "print(\"Every other column :\", a[:, ::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-index-md",
   "metadata": {},
   "source": [
    "The slice notation is `start:stop:step` along each dimension, separated by commas. Negative indices count from the end. `a[:, 1]` selects column 1 across all rows — the colon alone means \"all elements along this axis\". Slices return views, not copies. In practice this means you can write efficient code that operates on subregions of large arrays without duplicating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bool-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3, 7, -1, 12, -5, 9, 0, 4])\n",
    "\n",
    "mask = x > 0\n",
    "print(\"Mask           :\", mask)\n",
    "print(\"Positive values:\", x[mask])\n",
    "\n",
    "x[x < 0] = 0\n",
    "print(\"After zeroing negatives:\", x)\n",
    "\n",
    "compound = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n",
    "print(\"\\nValues between -3 and 3:\", compound[(compound > -3) & (compound < 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bool-index-md",
   "metadata": {},
   "source": [
    "Boolean indexing creates a mask — an array of True/False values — then uses it to select elements. This pattern replaces filtering loops entirely. In data preprocessing, you use it to clip negative pixel values, remove outliers, or select samples meeting some criterion. Note the use of `&` (bitwise AND) instead of Python's `and` — this is required because NumPy overloads bitwise operators element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[10, 20, 30],\n",
    "              [40, 50, 60],\n",
    "              [70, 80, 90],\n",
    "              [100,110,120]])\n",
    "\n",
    "print(\"Rows 0 and 2:\\n\", a[[0, 2]])\n",
    "print(\"\\nRows 1,3 and cols 0,2:\\n\", a[np.ix_([1, 3], [0, 2])])\n",
    "\n",
    "indices = np.array([2, 0, 3, 1])\n",
    "print(\"\\nRows in order [2,0,3,1]:\\n\", a[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-index-md",
   "metadata": {},
   "source": [
    "Fancy indexing uses an array of integers to select specific rows or columns. Unlike slices, fancy indexing always returns a copy. `np.ix_` constructs an open mesh from index arrays, allowing you to select the intersection of specific rows and columns. This is how mini-batch sampling works — you generate a random array of indices and index into your dataset array to extract that batch."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "math-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Element-wise Math and Universal Functions\n",
    "\n",
    "Every arithmetic operator in NumPy operates element-wise. NumPy also provides a library of universal functions (ufuncs) — vectorized functions that apply a mathematical operation to every element of an array efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elem-math",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "b = np.array([10.0, 20.0, 30.0, 40.0])\n",
    "\n",
    "print(\"a + b  :\", a + b)\n",
    "print(\"a - b  :\", a - b)\n",
    "print(\"a * b  :\", a * b)\n",
    "print(\"a / b  :\", a / b)\n",
    "print(\"a ** 2 :\", a ** 2)\n",
    "print(\"a % 3  :\", a % 3)\n",
    "print(\"a // 3 :\", a // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elem-math-md",
   "metadata": {},
   "source": [
    "These operators call underlying ufuncs: `np.add`, `np.multiply`, `np.power`, and so on. Writing `a + b` is identical to writing `np.add(a, b)`. The ufunc form is sometimes useful when you need the function as a first-class object, for example passing it as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufuncs",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.0, np.pi/6, np.pi/4, np.pi/2, np.pi])\n",
    "\n",
    "print(\"np.sin        :\", np.sin(x))\n",
    "print(\"np.cos        :\", np.cos(x))\n",
    "print(\"np.exp([0,1,2]):\", np.exp(np.array([0.0, 1.0, 2.0])))\n",
    "print(\"np.log([1,e,e2]):\", np.log(np.array([1.0, np.e, np.e**2])))\n",
    "print(\"np.log2([1,2,4]):\", np.log2(np.array([1.0, 2.0, 4.0])))\n",
    "print(\"np.sqrt      :\", np.sqrt(np.array([1.0, 4.0, 9.0, 16.0])))\n",
    "print(\"np.abs       :\", np.abs(np.array([-3, -2, -1, 0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ufuncs-md",
   "metadata": {},
   "source": [
    "These functions are the building blocks of activation functions, loss functions, and normalization. The sigmoid activation function is `1 / (1 + np.exp(-x))`. The softmax function requires `np.exp`. Log-loss requires `np.log`. Understanding that these are just element-wise operations on arrays demystifies a large part of neural network mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])\n",
    "\n",
    "sigmoid  = 1.0 / (1.0 + np.exp(-z))\n",
    "relu     = np.maximum(0, z)\n",
    "tanh     = np.tanh(z)\n",
    "\n",
    "exp_z    = np.exp(z)\n",
    "softmax  = exp_z / exp_z.sum()\n",
    "\n",
    "print(\"Input   :\", z)\n",
    "print(\"Sigmoid :\", np.round(sigmoid, 4))\n",
    "print(\"ReLU    :\", relu)\n",
    "print(\"Tanh    :\", np.round(tanh, 4))\n",
    "print(\"Softmax :\", np.round(softmax, 4))\n",
    "print(\"Softmax sums to:\", softmax.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activations-md",
   "metadata": {},
   "source": [
    "These are the actual activation functions used in neural networks, implemented from scratch using NumPy operations. Sigmoid squashes any input to `(0, 1)`, historically used in binary classification outputs. ReLU clips negative values to zero — currently the most popular hidden-layer activation. Softmax converts a vector of raw scores into a probability distribution (sums to 1), used in multi-class classification output layers. Deep learning frameworks implement these identically under the hood."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "broadcast-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Broadcasting\n",
    "\n",
    "Broadcasting is NumPy's mechanism for applying operations between arrays of different but compatible shapes. It avoids copying data and enables concise, efficient code. It is also one of the most misunderstood features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "print(\"Scalar broadcast (a + 10):\\n\", a + 10)\n",
    "\n",
    "row = np.array([10, 20, 30])\n",
    "print(\"\\nRow vector broadcast (a + row):\\n\", a + row)\n",
    "\n",
    "col = np.array([[10], [20], [30]])\n",
    "print(\"\\nCol vector broadcast (a + col):\\n\", a + col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadcast-1-md",
   "metadata": {},
   "source": [
    "Broadcasting rules: NumPy compares shapes from the right. Dimensions are compatible if they are equal or one of them is 1. A missing dimension is treated as 1. So a scalar has shape `()`, which is compatible with anything. A row vector of shape `(3,)` broadcasts against a matrix `(3, 3)` by repeating itself along the row axis. A column vector of shape `(3, 1)` broadcasts by repeating along the column axis.\n",
    "\n",
    "In practice: adding a bias term to every sample in a batch, normalizing each feature by its mean, or computing pairwise distances — all are broadcasting problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.random.randn(32, 10)\n",
    "mean  = batch.mean(axis=0)\n",
    "std   = batch.std(axis=0)\n",
    "\n",
    "normalized = (batch - mean) / std\n",
    "\n",
    "print(\"Batch shape      :\", batch.shape)\n",
    "print(\"Mean shape       :\", mean.shape)\n",
    "print(\"Normalized shape :\", normalized.shape)\n",
    "print(\"Normalized mean  :\", normalized.mean(axis=0).round(6))\n",
    "print(\"Normalized std   :\", normalized.std(axis=0).round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadcast-2-md",
   "metadata": {},
   "source": [
    "This is batch normalization in its simplest form. `mean` has shape `(10,)` which broadcasts against `(32, 10)` — NumPy subtracts each feature's mean from every sample's corresponding feature. The result has zero mean and unit variance per feature. This normalization step stabilizes training and appears in almost every serious deep learning pipeline."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "aggregation-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Aggregation and Reduction\n",
    "\n",
    "Reduction operations collapse one or more dimensions by applying a function across elements. The `axis` parameter controls which dimension gets collapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "print(\"Sum all     :\", x.sum())\n",
    "print(\"Sum axis=0  :\", x.sum(axis=0))\n",
    "print(\"Sum axis=1  :\", x.sum(axis=1))\n",
    "\n",
    "print(\"\\nMean all    :\", x.mean())\n",
    "print(\"Mean axis=0 :\", x.mean(axis=0))\n",
    "print(\"Mean axis=1 :\", x.mean(axis=1))\n",
    "\n",
    "print(\"\\nMax all     :\", x.max())\n",
    "print(\"Argmax all  :\", x.argmax())\n",
    "print(\"Argmax axis=1:\", x.argmax(axis=1))\n",
    "\n",
    "print(\"\\nStd axis=0  :\", x.std(axis=0))\n",
    "print(\"Cumsum axis=1:\\n\", x.cumsum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregation-md2",
   "metadata": {},
   "source": [
    "`axis=0` collapses rows (aggregates down each column). `axis=1` collapses columns (aggregates across each row). Thinking about axis as \"the dimension that disappears\" helps: summing shape `(3, 3)` along `axis=0` produces shape `(3,)` — the row dimension was consumed.\n",
    "\n",
    "`argmax` returns the index of the maximum value, not the value itself. This is how you convert network output probabilities to predicted class labels: `predicted_class = logits.argmax(axis=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keepdims",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(3, 4)\n",
    "\n",
    "mean_no_keep = x.mean(axis=1)\n",
    "mean_keep    = x.mean(axis=1, keepdims=True)\n",
    "\n",
    "print(\"Shape without keepdims:\", mean_no_keep.shape)\n",
    "print(\"Shape with keepdims   :\", mean_keep.shape)\n",
    "\n",
    "try:\n",
    "    centered_bad = x - mean_no_keep\n",
    "except ValueError as e:\n",
    "    print(\"\\nError without keepdims:\", e)\n",
    "\n",
    "centered_good = x - mean_keep\n",
    "print(\"Centered shape with keepdims:\", centered_good.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keepdims-md",
   "metadata": {},
   "source": [
    "`keepdims=True` preserves the collapsed dimension as a size-1 dimension. This keeps the result broadcastable against the original array. Without it, subtracting a shape `(3,)` mean from a shape `(3, 4)` matrix fails because the shapes are not compatible. With `keepdims=True`, the mean has shape `(3, 1)` and broadcasts correctly. This is a common source of shape errors."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "linalg-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Algebra\n",
    "\n",
    "Linear algebra is the language of machine learning. Matrix multiplication, dot products, eigendecomposition, and norms appear throughout optimization theory, dimensionality reduction, and attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linalg-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]], dtype=float)\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]], dtype=float)\n",
    "\n",
    "C = A @ B\n",
    "print(\"A shape:\", A.shape, \"B shape:\", B.shape)\n",
    "print(\"A @ B shape:\", C.shape)\n",
    "print(\"A @ B:\\n\", C)\n",
    "\n",
    "print(\"\\nElement-wise A * A.T would fail (wrong shapes)\")\n",
    "D = A.T @ A\n",
    "print(\"A.T @ A shape:\", D.shape)\n",
    "print(\"A.T @ A:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linalg-1-md",
   "metadata": {},
   "source": [
    "The `@` operator is matrix multiplication. `A @ B` requires the inner dimensions to match: `(m, k) @ (k, n)` produces `(m, n)`. A forward pass through a linear layer is exactly `output = X @ W.T + b` where `X` is the input batch, `W` is the weight matrix, and `b` is the bias. `A.T @ A` produces a symmetric positive semi-definite matrix — this structure appears in normal equations, covariance matrices, and the Gram matrix used in style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linalg-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1.0, 2.0, 3.0])\n",
    "v = np.array([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"Dot product        :\", np.dot(u, v))\n",
    "\n",
    "print(\"L1 norm (sum |x|)  :\", np.linalg.norm(u, ord=1))\n",
    "print(\"L2 norm (Euclidean):\", np.linalg.norm(u))\n",
    "print(\"L2 norm manual     :\", np.sqrt((u**2).sum()))\n",
    "\n",
    "u_unit = u / np.linalg.norm(u)\n",
    "print(\"Unit vector        :\", u_unit)\n",
    "print(\"Unit vector norm   :\", np.linalg.norm(u_unit))\n",
    "\n",
    "cos_sim = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "print(\"\\nCosine similarity  :\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linalg-2-md",
   "metadata": {},
   "source": [
    "The L2 norm measures the length of a vector. Dividing a vector by its norm gives a unit vector — this is how you normalize word embeddings or gradient vectors. L1 regularization penalizes the L1 norm of weights, L2 regularization penalizes the L2 norm squared. Cosine similarity measures the angle between two vectors regardless of their magnitude — this is the primary similarity metric used in semantic search and recommendation systems with embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linalg-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 1],\n",
    "              [1, 3]], dtype=float)\n",
    "\n",
    "det  = np.linalg.det(A)\n",
    "inv  = np.linalg.inv(A)\n",
    "vals, vecs = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nDeterminant:\", det)\n",
    "print(\"Inverse:\\n\", inv)\n",
    "print(\"\\nA @ inv(A) (should be identity):\\n\", (A @ inv).round(6))\n",
    "print(\"\\nEigenvalues :\", vals)\n",
    "print(\"Eigenvectors:\\n\", vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linalg-3-md",
   "metadata": {},
   "source": [
    "The determinant tells you whether a matrix is invertible (non-zero) and how it scales area or volume. Eigenvalues and eigenvectors describe which directions a matrix stretches and by how much — this is the foundation of Principal Component Analysis (PCA), which reduces the dimensionality of data by projecting onto the directions of maximum variance. The eigenvectors become the new coordinate axes; the eigenvalues tell you how much variance each axis captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(5, 3)\n",
    "\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "print(\"X shape :\", X.shape)\n",
    "print(\"U shape :\", U.shape)\n",
    "print(\"S shape :\", S.shape)\n",
    "print(\"Vt shape:\", Vt.shape)\n",
    "\n",
    "X_reconstructed = U @ np.diag(S) @ Vt\n",
    "print(\"\\nReconstruction error:\", np.abs(X - X_reconstructed).max())\n",
    "\n",
    "X_compressed = U[:, :2] @ np.diag(S[:2]) @ Vt[:2, :]\n",
    "print(\"Compressed shape:\", X_compressed.shape)\n",
    "print(\"Compression error:\", np.abs(X - X_compressed).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svd-md",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) factorizes any matrix into `U @ diag(S) @ Vt`. Keeping only the top-k singular values and corresponding vectors gives the best rank-k approximation to the original matrix — this is the mathematical foundation of image compression, recommendation systems (matrix factorization), and the modern Transformer attention mechanism's low-rank approximation techniques."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "stacking-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Concatenation, Stacking, and Splitting\n",
    "\n",
    "Data pipelines frequently need to combine arrays from different sources or split arrays for batching and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3]])\n",
    "b = np.array([[4, 5, 6]])\n",
    "\n",
    "cat_row = np.concatenate([a, b], axis=0)\n",
    "cat_col = np.concatenate([a, b], axis=1)\n",
    "vstack  = np.vstack([a, b])\n",
    "hstack  = np.hstack([a, b])\n",
    "stacked = np.stack([a, b], axis=0)\n",
    "\n",
    "print(\"concatenate axis=0:\\n\", cat_row, \"shape:\", cat_row.shape)\n",
    "print(\"concatenate axis=1:\\n\", cat_col, \"shape:\", cat_col.shape)\n",
    "print(\"vstack:\\n\", vstack, \"shape:\", vstack.shape)\n",
    "print(\"hstack:\\n\", hstack, \"shape:\", hstack.shape)\n",
    "print(\"stack axis=0 shape:\", stacked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stacking-md2",
   "metadata": {},
   "source": [
    "`concatenate` joins arrays along an existing axis. `stack` creates a new axis before joining. `vstack` is shorthand for `concatenate(axis=0)`. `hstack` is `concatenate(axis=1)` for 2D arrays. The critical distinction: concatenating two `(1, 3)` arrays along axis=0 gives `(2, 3)`; stacking them along axis=0 gives `(2, 1, 3)`. In batch assembly, you typically collect individual samples as 1D arrays and stack them to form a 2D batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "splitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(40).reshape(8, 5)\n",
    "\n",
    "train, val, test = np.split(data, [6, 7], axis=0)\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Val shape  :\", val.shape)\n",
    "print(\"Test shape :\", test.shape)\n",
    "\n",
    "chunks = np.array_split(data, 3, axis=0)\n",
    "print(\"\\narray_split into 3 unequal parts:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"  Chunk {i}: shape {chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "splitting-md",
   "metadata": {},
   "source": [
    "`np.split` takes indices at which to cut. `np.split(data, [6, 7])` makes cuts before index 6 and before index 7, producing three parts: rows 0-5, row 6, and row 7. `np.array_split` is more forgiving — it accepts a number and splits as evenly as possible, with some chunks having one more element than others when it does not divide evenly. This is the workhorse for train/validation/test splitting."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "sorting-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Sorting and Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorting",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3])\n",
    "\n",
    "print(\"Original   :\", x)\n",
    "print(\"Sorted     :\", np.sort(x))\n",
    "print(\"Argsort    :\", np.argsort(x))\n",
    "print(\"Top-3 vals :\", np.sort(x)[-3:])\n",
    "print(\"Top-3 idx  :\", np.argsort(x)[-3:])\n",
    "\n",
    "logits = np.array([0.1, 2.5, -0.3, 1.8, 3.2, 0.7])\n",
    "top_k = 3\n",
    "top_k_idx = np.argsort(logits)[-top_k:][::-1]\n",
    "print(\"\\nClass logits :\", logits)\n",
    "print(\"Top-3 classes:\", top_k_idx)\n",
    "print(\"Top-3 values :\", logits[top_k_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorting-md2",
   "metadata": {},
   "source": [
    "`argsort` returns the indices that would sort the array — not the sorted values themselves. This is valuable because you can use those indices to reorder other arrays consistently. The top-k prediction pattern shown above — sort logits by value, take the last k indices, reverse to get descending order — is used in evaluation metrics like top-5 accuracy (used to evaluate ImageNet models)."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "practical-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Putting It Together: A Complete Data Preprocessing Pipeline\n",
    "\n",
    "This section combines everything covered above into a realistic workflow: generating synthetic data, splitting it, normalizing it using only training statistics, and verifying the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "n_samples  = 1000\n",
    "n_features = 8\n",
    "\n",
    "X = rng.normal(loc=5.0, scale=2.0, size=(n_samples, n_features))\n",
    "y = (X[:, 0] + X[:, 1] > 10).astype(np.int64)\n",
    "\n",
    "n_train = int(0.7 * n_samples)\n",
    "n_val   = int(0.15 * n_samples)\n",
    "\n",
    "idx = rng.permutation(n_samples)\n",
    "train_idx = idx[:n_train]\n",
    "val_idx   = idx[n_train:n_train + n_val]\n",
    "test_idx  = idx[n_train + n_val:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val,   y_val   = X[val_idx],   y[val_idx]\n",
    "X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "std  = X_train.std(axis=0)\n",
    "\n",
    "X_train_norm = (X_train - mean) / std\n",
    "X_val_norm   = (X_val   - mean) / std\n",
    "X_test_norm  = (X_test  - mean) / std\n",
    "\n",
    "print(\"Dataset shape            :\", X.shape)\n",
    "print(\"Train / Val / Test sizes :\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
    "print(\"Class balance (train)    :\", y_train.mean().round(3))\n",
    "print(\"\\nTrain norm mean (should be ~0):\", X_train_norm.mean(axis=0).round(3))\n",
    "print(\"Train norm std  (should be ~1):\", X_train_norm.std(axis=0).round(3))\n",
    "print(\"\\nVal   norm mean (approx  ~0)  :\", X_val_norm.mean(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-md",
   "metadata": {},
   "source": [
    "Several important design decisions are embedded here. Shuffling with `rng.permutation` before splitting prevents accidentally creating splits where one set contains only early or only late data. Normalization statistics are computed on training data only and applied to validation and test sets — if you use the full dataset statistics, you leak information about the test set into your normalization, which inflates evaluation scores. This is called data leakage and it is a common mistake in machine learning practice."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Reference: NumPy Vocabulary\n",
    "\n",
    "| Concept | Code | Notes |\n",
    "|---|---|---|\n",
    "| Create from list | `np.array([1,2,3])` | Infers dtype |\n",
    "| Zeros | `np.zeros((m, n))` | Default float64 |\n",
    "| Range | `np.arange(start, stop, step)` | Stop is exclusive |\n",
    "| Evenly spaced | `np.linspace(start, stop, num)` | Stop is inclusive |\n",
    "| Random uniform | `rng.random((m, n))` | Use `default_rng` |\n",
    "| Shape | `a.shape` | Returns a tuple |\n",
    "| Type | `a.dtype` | float64 default |\n",
    "| Cast | `a.astype(np.float32)` | Returns copy |\n",
    "| Reshape | `a.reshape(m, n)` | Returns view |\n",
    "| Transpose | `a.T` | Returns view |\n",
    "| Add dim | `np.expand_dims(a, axis)` | Same as `a[np.newaxis]` |\n",
    "| Remove dim | `np.squeeze(a)` | Removes size-1 dims |\n",
    "| Flatten | `a.flatten()` | Returns copy |\n",
    "| Slice | `a[0:2, 1:3]` | Returns view |\n",
    "| Boolean mask | `a[a > 0]` | Returns copy |\n",
    "| Matrix multiply | `A @ B` | Also `np.matmul` |\n",
    "| Element multiply | `A * B` | Hadamard product |\n",
    "| Dot product | `np.dot(u, v)` | For 1D vectors |\n",
    "| Sum | `a.sum(axis=0)` | axis=0 collapses rows |\n",
    "| Argmax | `a.argmax(axis=1)` | Index of max |\n",
    "| L2 norm | `np.linalg.norm(a)` | Euclidean length |\n",
    "| SVD | `np.linalg.svd(A)` | Factorization |\n",
    "| Concatenate | `np.concatenate([a,b], axis=0)` | Existing axis |\n",
    "| Stack | `np.stack([a,b], axis=0)` | New axis |\n",
    "| Copy | `a.copy()` | Independent data |\n",
    "| Where | `np.where(cond, x, y)` | Element-wise ternary |"
   ]
  }

 ]
}
